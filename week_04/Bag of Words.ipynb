{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words (Or: *What is unique about working with text*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: How is text different from the types of data you've seen so far?\n",
    "* It's a string, so can't do mathematical operations on it\n",
    "* It can be ambigious\n",
    "* It can be in a wide range of formats / files\n",
    "* Has a wide granularity level (file, paragraph, sentence, word, character)\n",
    "* It is unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What does this mean for how you can work with text compared to types of data you've worked with so far?\n",
    "* Need to make it structured\n",
    "* Need to clean it well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main aspect of working with text that helps us feed it into computers / machine learning models:\n",
    "1. Data/text preprocessing\n",
    "2. Turning text into features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already know that data cleaning is a large part of a data scientist's workflow. When working with unstructured data such as text, data cleaning plays an even bigger role.\n",
    "\n",
    "**Q**: Things we may want to do to clean our textual data:\n",
    "* Lemmatization\n",
    "* Stemming\n",
    "* Tokenization\n",
    "* Remove small words that don't contribute to the meaning\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split the corpus into individual words / tokens\n",
    "* Remove punctuation\n",
    "* Deal with capitalization\n",
    "* Remove most common words:\n",
    "    * Use a list of words to remove (stop words)\n",
    "    * Remove the words that appear in more than X% of documents\n",
    "* Reduce words to their base parts:\n",
    "    * *Stemming*: process of removing and replacing suffixes to get to the root of the words, *stem*.\n",
    "        * based on heuristics (e.g. *-ational -> -ate*, *-tional -> -tion*)\n",
    "        * does not always produce a word\n",
    "        * feet->feet, wolves->wolv, cats->cat, talked->talk\n",
    "    * *Lemmatization*: uses vocabulary and morphological analysis to return the base or dictionary form of a word, *lemma*.\n",
    "        * does not always return a reduced form\n",
    "        * feet->foot, wolves->wolf, cats->cat, talked->talked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"we all love a yellow submarine\",             # Beatles\n",
    "          \"yesterday, my submarine was in love\",        # Beatles\n",
    "          \"we are love trouble with loyalty here\",      # Eminem\n",
    "          \"loyalty to us is worth more than love is\"]   # Eminem\n",
    "labels = ['Beatles'] * 2 + ['Eminem'] * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Turning text into features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Once you have your data cleaned and preprocessed like this, what can you imagine using as your features?\n",
    "* Amount of certain words\n",
    "* Length of song / words in song\n",
    "* Categorize words, then use word count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token/word will be a feature/column.\n",
    "* large sparse matrix\n",
    "* word order lost\n",
    "* counts not normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x20 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 26 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "      <th>are</th>\n",
       "      <th>here</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>loyalty</th>\n",
       "      <th>more</th>\n",
       "      <th>my</th>\n",
       "      <th>submarine</th>\n",
       "      <th>than</th>\n",
       "      <th>to</th>\n",
       "      <th>trouble</th>\n",
       "      <th>us</th>\n",
       "      <th>was</th>\n",
       "      <th>we</th>\n",
       "      <th>with</th>\n",
       "      <th>worth</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yesterday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Beatles</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beatles</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eminem</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eminem</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         all  are  here  in  is  love  loyalty  more  my  submarine  than  to  \\\n",
       "Beatles    1    0     0   0   0     1        0     0   0          1     0   0   \n",
       "Beatles    0    0     0   1   0     1        0     0   1          1     0   0   \n",
       "Eminem     0    1     1   0   0     1        1     0   0          0     0   0   \n",
       "Eminem     0    0     0   0   2     1        1     1   0          0     1   1   \n",
       "\n",
       "         trouble  us  was  we  with  worth  yellow  yesterday  \n",
       "Beatles        0   0    0   1     0      0       1          0  \n",
       "Beatles        0   0    1   0     0      0       0          1  \n",
       "Eminem         1   0    0   1     1      0       0          0  \n",
       "Eminem         0   1    0   0     0      1       0          0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names(), index=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise: How can we remove the most common words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using a list of stop words\n",
    "* Removing the words that appear in more than X% of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `CountVectorizer` documentation for how to do each of these: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove most common words using these two methods. Use `.vocabulary_` and `.stop_words_` attributes to see which words have remained and which are removed (latter only in the case of the second method). \n",
    "\n",
    "What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using a list of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X_df = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names(), index=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>love</th>\n",
       "      <th>loyalty</th>\n",
       "      <th>submarine</th>\n",
       "      <th>trouble</th>\n",
       "      <th>worth</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yesterday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Beatles</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beatles</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eminem</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eminem</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         love  loyalty  submarine  trouble  worth  yellow  yesterday\n",
       "Beatles     1        0          1        0      0       1          0\n",
       "Beatles     1        0          1        0      0       0          1\n",
       "Eminem      1        1          0        1      0       0          0\n",
       "Eminem      1        1          0        0      1       0          0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Removing the words that appear in more than X% of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_df=0.75)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X_df = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names(), index=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 43)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['all', 'are', 'here', 'in', 'is', 'loyalty', 'more', 'my', 'submarine',\n",
       "       'than', 'to', 'trouble', 'us', 'was', 'we', 'with', 'worth', 'yellow',\n",
       "       'yesterday'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'we': 14,\n",
       " 'all': 0,\n",
       " 'yellow': 17,\n",
       " 'submarine': 8,\n",
       " 'yesterday': 18,\n",
       " 'my': 7,\n",
       " 'was': 13,\n",
       " 'in': 3,\n",
       " 'are': 1,\n",
       " 'trouble': 11,\n",
       " 'with': 15,\n",
       " 'loyalty': 5,\n",
       " 'here': 2,\n",
       " 'to': 10,\n",
       " 'us': 12,\n",
       " 'is': 4,\n",
       " 'worth': 16,\n",
       " 'more': 6,\n",
       " 'than': 9}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'are',\n",
       " 'here',\n",
       " 'in',\n",
       " 'is',\n",
       " 'loyalty',\n",
       " 'more',\n",
       " 'my',\n",
       " 'submarine',\n",
       " 'than',\n",
       " 'to',\n",
       " 'trouble',\n",
       " 'us',\n",
       " 'was',\n",
       " 'we',\n",
       " 'with',\n",
       " 'worth',\n",
       " 'yellow',\n",
       " 'yesterday']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of single tokens, we now also count token pairs (bigrams), triplets (trigrams), etc.\n",
    "* even larger sparser matrix\n",
    "* preserves local word order\n",
    "* counts not normalized\n",
    "* too many features:\n",
    "    * remove high-frequency n-grams: can include stop words; not very informative\n",
    "    * remove low-frequency n-grams: typos and rare n-grams; likely to overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X_df = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names(), index=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "      <th>all love</th>\n",
       "      <th>are</th>\n",
       "      <th>are love</th>\n",
       "      <th>here</th>\n",
       "      <th>in</th>\n",
       "      <th>in love</th>\n",
       "      <th>is</th>\n",
       "      <th>is worth</th>\n",
       "      <th>love</th>\n",
       "      <th>...</th>\n",
       "      <th>we all</th>\n",
       "      <th>we are</th>\n",
       "      <th>with</th>\n",
       "      <th>with loyalty</th>\n",
       "      <th>worth</th>\n",
       "      <th>worth more</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yellow submarine</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yesterday my</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Beatles</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beatles</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eminem</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eminem</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         all  all love  are  are love  here  in  in love  is  is worth  love  \\\n",
       "Beatles    1         1    0         0     0   0        0   0         0     1   \n",
       "Beatles    0         0    0         0     0   1        1   0         0     1   \n",
       "Eminem     0         0    1         1     1   0        0   0         0     1   \n",
       "Eminem     0         0    0         0     0   0        0   2         1     1   \n",
       "\n",
       "         ...  we all  we are  with  with loyalty  worth  worth more  yellow  \\\n",
       "Beatles  ...       1       0     0             0      0           0       1   \n",
       "Beatles  ...       0       0     0             0      0           0       0   \n",
       "Eminem   ...       0       1     1             1      0           0       0   \n",
       "Eminem   ...       0       0     0             0      1           1       0   \n",
       "\n",
       "         yellow submarine  yesterday  yesterday my  \n",
       "Beatles                 1          0             0  \n",
       "Beatles                 0          1             1  \n",
       "Eminem                  0          0             0  \n",
       "Eminem                  0          0             0  \n",
       "\n",
       "[4 rows x 43 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stands for `term frequency - inverse document frequency` and aims to address the popularity/frequency of words in a corpus(not just inside of a single document)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF = term frequency \n",
    "\n",
    "TF(t, d) - frequency of term (n-gram) _t_ in document _d_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IDF(t) = inverse document frequency (of term _t_ in the whole corpus)\n",
    "\n",
    "$ IDF(t) = \\log \\frac{1+N}{1+N_t}+1 $\n",
    "\n",
    "\n",
    "If term _t_ doesn't appear in many documents: IDF is \"big\".\n",
    "\n",
    "If term _t_ appears in many documents: IDF is close to 1 (\"small\") -> common terms are penalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ TFIDF(t, d) = TF(t,d)*IDF(t) $ \n",
    "\n",
    "**Q**: What kind of terms will have high TF-IDF?\n",
    "* Those that appear a lot in small number of documents/songs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise: Implement TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look up how to implement TF-IDF vectorizer in `scikit-learn`. How does your features dataframe differ from the `CountVectorizer` one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X_df = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names(), index=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "      <th>are</th>\n",
       "      <th>here</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>loyalty</th>\n",
       "      <th>more</th>\n",
       "      <th>my</th>\n",
       "      <th>submarine</th>\n",
       "      <th>than</th>\n",
       "      <th>to</th>\n",
       "      <th>trouble</th>\n",
       "      <th>us</th>\n",
       "      <th>was</th>\n",
       "      <th>we</th>\n",
       "      <th>with</th>\n",
       "      <th>worth</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yesterday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Beatles</th>\n",
       "      <td>0.533343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.278320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.533343</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beatles</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.452035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.235891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.452035</td>\n",
       "      <td>0.356389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.452035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.452035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eminem</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425802</td>\n",
       "      <td>0.425802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222201</td>\n",
       "      <td>0.335707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335707</td>\n",
       "      <td>0.425802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eminem</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.635837</td>\n",
       "      <td>0.165903</td>\n",
       "      <td>0.250651</td>\n",
       "      <td>0.317919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317919</td>\n",
       "      <td>0.317919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              all       are      here        in        is      love   loyalty  \\\n",
       "Beatles  0.533343  0.000000  0.000000  0.000000  0.000000  0.278320  0.000000   \n",
       "Beatles  0.000000  0.000000  0.000000  0.452035  0.000000  0.235891  0.000000   \n",
       "Eminem   0.000000  0.425802  0.425802  0.000000  0.000000  0.222201  0.335707   \n",
       "Eminem   0.000000  0.000000  0.000000  0.000000  0.635837  0.165903  0.250651   \n",
       "\n",
       "             more        my  submarine      than        to   trouble  \\\n",
       "Beatles  0.000000  0.000000   0.420493  0.000000  0.000000  0.000000   \n",
       "Beatles  0.000000  0.452035   0.356389  0.000000  0.000000  0.000000   \n",
       "Eminem   0.000000  0.000000   0.000000  0.000000  0.000000  0.425802   \n",
       "Eminem   0.317919  0.000000   0.000000  0.317919  0.317919  0.000000   \n",
       "\n",
       "               us       was        we      with     worth    yellow  yesterday  \n",
       "Beatles  0.000000  0.000000  0.420493  0.000000  0.000000  0.533343   0.000000  \n",
       "Beatles  0.000000  0.452035  0.000000  0.000000  0.000000  0.000000   0.452035  \n",
       "Eminem   0.000000  0.000000  0.335707  0.425802  0.000000  0.000000   0.000000  \n",
       "Eminem   0.317919  0.000000  0.000000  0.000000  0.317919  0.000000   0.000000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bonus question*: What can you say about the values in your new `X_df` (think about sums, normalizations, etc.)? Post your guesses in Slack! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your scraped lyrics and run them through a vectorizer of your choice. Then split the resulting feature vector `X` and your labels into training and test set and train a logistic regression model. Check how the choice of vectorizers and the parameters we mentioned affects the performance of your model.\n",
    "\n",
    "Once you have your final model, run the following lines of code to get the words that are strongest predictors for each of your bands and post them in Slack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`import operator`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model = LogisticRegression()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`print(operator.itemgetter(*np.argsort(model.coef_[0]))(vectorizer.get_feature_names())[-20:])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`print(operator.itemgetter(*np.argsort(model.coef_[0]))(vectorizer.get_feature_names())[:20])`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
